use crate::ai::{AIConfig, ChatMessage, OpenAIResponse};
use crate::error::AppError;
use crate::tools;
use serde_json::json;
use std::collections::HashMap;
use std::sync::atomic::{AtomicBool, Ordering};
use std::sync::{Mutex, OnceLock};
use std::time::Duration;
use tauri::{AppHandle, Emitter};

/// æµå¼çŠ¶æ€ç®¡ç†ï¼šä½¿ç”¨ request_id ä½œä¸º keyï¼Œæ”¯æŒå¤šä¸ªå¹¶å‘æµç‹¬ç«‹æ§åˆ¶
static STREAM_STATES: OnceLock<Mutex<HashMap<String, AtomicBool>>> = OnceLock::new();

fn get_stream_states() -> &'static Mutex<HashMap<String, AtomicBool>> {
    STREAM_STATES.get_or_init(|| Mutex::new(HashMap::new()))
}

/// æµå¤„ç† Buffer æœ€å¤§é™åˆ¶ï¼ˆ10MBï¼‰ï¼Œé˜²æ­¢æ¶æ„æœåŠ¡å™¨å‘é€æ— é™æ•°æ®
const MAX_BUFFER_SIZE: usize = 10 * 1024 * 1024;

#[tauri::command]
pub fn stop_ai_stream(request_id: Option<String>) {
    let states = get_stream_states();
    if let Some(id) = request_id {
        // åœæ­¢ç‰¹å®šçš„æµ
        if let Ok(states) = states.lock() {
            if let Some(cancelled) = states.get(&id) {
                cancelled.store(true, Ordering::SeqCst);
            }
        }
    } else {
        // åœæ­¢æ‰€æœ‰æµï¼ˆå‘åå…¼å®¹ï¼‰
        if let Ok(states) = states.lock() {
            for cancelled in states.values() {
                cancelled.store(true, Ordering::SeqCst);
            }
        }
    }
}

/// æ¸…ç†å·²å®Œæˆçš„æµ
fn cleanup_stream(request_id: &str) {
    let states = get_stream_states();
    if let Ok(mut states) = states.lock() {
        states.remove(request_id);
    }
}

/// æ£€æŸ¥æµæ˜¯å¦è¢«å–æ¶ˆ
fn is_stream_cancelled(request_id: &str) -> bool {
    let states = get_stream_states();
    if let Ok(states) = states.lock() {
        if let Some(cancelled) = states.get(request_id) {
            return cancelled.load(Ordering::SeqCst);
        }
    }
    false
}

type Result<T> = std::result::Result<T, AppError>;

#[tauri::command]
pub async fn chat(
    app: AppHandle,
    messages: Vec<ChatMessage>,
    provider: Option<String>,
    api_key: Option<String>,
    model: Option<String>,
    base_url: Option<String>,
    temperature: Option<f64>,
    max_tokens: Option<u32>,
    enable_web_search: Option<bool>,
) -> Result<String> {
    let config = get_ai_config(&app, provider, api_key, model, base_url);
    let web_search = enable_web_search.unwrap_or(false);
    let client = reqwest::Client::new();

    // OpenAI + è”ç½‘æœç´¢ â†’ Responses APIï¼ˆéæµå¼ï¼‰
    if config.provider == "openai" && web_search {
        return call_openai_responses(&config, &client, &messages, max_tokens).await;
    }

    // Anthropic + è”ç½‘æœç´¢ â†’ Anthropic Messages APIï¼ˆéæµå¼ï¼‰
    if config.provider == "anthropic" && web_search {
        return call_anthropic_with_search(&config, &client, &messages, max_tokens).await;
    }

    let mut request_body = json!({
        "messages": messages,
        "model": config.get_default_model(),
        "temperature": temperature.unwrap_or(0.7),
        "stream": false
    });

    if let Some(mt) = max_tokens {
        request_body["max_tokens"] = json!(mt);
    }

    // è”ç½‘æœç´¢ï¼šæ ¹æ® provider æ³¨å…¥æ­£ç¡®çš„å‚æ•°æ ¼å¼
    if web_search {
        inject_web_search_params(&mut request_body, &config);
    }

    let url = format!("{}/chat/completions", config.get_base_url());

    let mut request_builder = client.post(&url).json(&request_body);

    // Set API key based on provider
    if let Some(key) = config.api_key {
        match config.provider.as_str() {
            "anthropic" => {
                request_builder = request_builder.header("x-api-key", key);
            }
            _ => {
                request_builder = request_builder.header("Authorization", format!("Bearer {}", key));
            }
        }
    }

    let response = request_builder
        .header("Content-Type", "application/json")
        .timeout(Duration::from_secs(120))
        .send()
        .await
        .map_err(|e| AppError::AIError(format!("Failed to connect to AI service: {}", e)))?;

    if !response.status().is_success() {
        let status = response.status();
        let error_text = response
            .text()
            .await
            .unwrap_or_else(|_| "Unknown error".to_string());
        return Err(AppError::AIError(format!(
            "AI API error ({}): {}",
            status, error_text
        )));
    }

    let openai_response: OpenAIResponse = response
        .json()
        .await
        .map_err(|e| AppError::AIError(format!("Failed to parse response: {}", e)))?;

    match openai_response {
        OpenAIResponse::Chat(resp) => {
            let content = resp
                .choices
                .first()
                .and_then(|c| c.message.as_ref())
                .map(|m| m.content.clone())
                .unwrap_or_default();

            Ok(content)
        }
        OpenAIResponse::Stream(_) => Err(AppError::AIError(
            "Unexpected stream response in non-stream mode".to_string(),
        )),
    }
}

#[tauri::command]
pub async fn chat_stream(
    app: AppHandle,
    messages: Vec<ChatMessage>,
    provider: Option<String>,
    api_key: Option<String>,
    model: Option<String>,
    base_url: Option<String>,
    window: tauri::Window,
    enable_web_search: Option<bool>,
    enable_thinking: Option<bool>,
    enable_tools: Option<bool>,
    project_documents: Option<Vec<serde_json::Value>>,
    request_id: Option<String>,
) -> Result<String> {
    let req_id = request_id.clone().unwrap_or_default();

    // æ³¨å†Œæ–°çš„æµ
    if let Ok(mut states) = get_stream_states().lock() {
        states.insert(req_id.clone(), AtomicBool::new(false));
    }

    // ç¡®ä¿åœ¨å‡½æ•°é€€å‡ºæ—¶æ¸…ç†æµçŠ¶æ€
    struct StreamGuard {
        request_id: String,
    }
    impl Drop for StreamGuard {
        fn drop(&mut self) {
            cleanup_stream(&self.request_id);
        }
    }
    let _guard = StreamGuard { request_id: req_id.clone() };

    let config = get_ai_config(&app, provider, api_key, model, base_url);
    let web_search = enable_web_search.unwrap_or(false);
    let use_tools = enable_tools.unwrap_or(false);

    // OpenAI + è”ç½‘æœç´¢ â†’ Responses API
    if config.provider == "openai" && web_search {
        return stream_openai_responses(&config, &messages, &req_id, &window).await;
    }

    // Anthropic + è”ç½‘æœç´¢ â†’ Anthropic Messages APIï¼ˆåŸç”Ÿæ ¼å¼ï¼‰
    if config.provider == "anthropic" && web_search {
        return stream_anthropic_with_search(&config, &messages, &req_id, &window).await;
    }

    let client = reqwest::Client::new();
    let url = format!("{}/chat/completions", config.get_base_url());
    let docs = project_documents.unwrap_or_default();

    // Function Calling å¾ªç¯ï¼šå…ˆç”¨éæµå¼æ£€æµ‹ tool_callsï¼Œæ‰§è¡Œå·¥å…·åå†æ¬¡è°ƒç”¨
    let mut current_messages: Vec<serde_json::Value> = messages.iter().map(|m| {
        json!({ "role": m.role, "content": m.content })
    }).collect();

    if use_tools {
        let tool_defs = tools::get_builtin_tool_definitions();
        let max_rounds = 5;

        for _round in 0..max_rounds {
            if is_stream_cancelled(&req_id) { break; }

            let mut tool_request = json!({
                "messages": current_messages,
                "model": config.get_default_model(),
                "temperature": 0.7,
                "stream": false,
                "tools": tool_defs
            });

            if web_search {
                inject_web_search_params(&mut tool_request, &config);
            }

            let mut req_builder = client
                .post(&url)
                .header("Content-Type", "application/json")
                .json(&tool_request);

            if let Some(key) = &config.api_key {
                match config.provider.as_str() {
                    "anthropic" => { req_builder = req_builder.header("x-api-key", key); }
                    _ => { req_builder = req_builder.header("Authorization", format!("Bearer {}", key)); }
                }
            }

            let resp = req_builder
                .timeout(Duration::from_secs(120))
                .send()
                .await
                .map_err(|e| AppError::AIError(format!("Tool call failed: {}", e)))?;

            if !resp.status().is_success() {
                let status = resp.status();
                let err = resp.text().await.unwrap_or_default();
                return Err(AppError::AIError(format!("Tool call error ({}): {}", status, err)));
            }

            let json_resp: serde_json::Value = resp.json().await
                .map_err(|e| AppError::AIError(format!("Parse tool response failed: {}", e)))?;

            let choice = json_resp.get("choices")
                .and_then(|c| c.get(0));

            let finish_reason = choice
                .and_then(|c| c.get("finish_reason"))
                .and_then(|f| f.as_str())
                .unwrap_or("");

            if finish_reason != "tool_calls" {
                // AI æ²¡æœ‰è¯·æ±‚å·¥å…·è°ƒç”¨ï¼Œè·³å‡ºå¾ªç¯è¿›å…¥æµå¼è¾“å‡º
                break;
            }

            // æå– tool_calls å¹¶æ‰§è¡Œ
            let tool_calls = choice
                .and_then(|c| c.get("message"))
                .and_then(|m| m.get("tool_calls"))
                .and_then(|tc| tc.as_array());

            if let Some(calls) = tool_calls {
                // å°† assistant æ¶ˆæ¯ï¼ˆå« tool_callsï¼‰åŠ å…¥å¯¹è¯
                if let Some(assistant_msg) = choice.and_then(|c| c.get("message")) {
                    current_messages.push(assistant_msg.clone());
                }

                // é€šçŸ¥å‰ç«¯æ­£åœ¨æ‰§è¡Œå·¥å…·
                let _ = window.emit("ai:stream:chunk", json!({
                    "request_id": req_id,
                    "content": "\n\n> ğŸ”§ æ­£åœ¨è°ƒç”¨å·¥å…·...\n\n"
                }));

                for call_val in calls {
                    let tool_call: tools::ToolCall = match serde_json::from_value(call_val.clone()) {
                        Ok(tc) => tc,
                        Err(_) => continue,
                    };

                    let result = tools::execute_tool(&tool_call, &docs);

                    // å°†å·¥å…·ç»“æœåŠ å…¥å¯¹è¯
                    current_messages.push(json!({
                        "role": "tool",
                        "tool_call_id": result.tool_call_id,
                        "content": result.content
                    }));
                }
            } else {
                break;
            }
        }
    }

    // æœ€ç»ˆæµå¼è¾“å‡º
    let mut request_body = json!({
        "messages": current_messages,
        "model": config.get_default_model(),
        "temperature": 0.7,
        "stream": true
    });

    // è”ç½‘æœç´¢ï¼šæ ¹æ® provider æ³¨å…¥æ­£ç¡®çš„å‚æ•°æ ¼å¼
    if web_search {
        inject_web_search_params(&mut request_body, &config);
    }

    // æ·±åº¦æ€è€ƒï¼šæ ¹æ® provider æ³¨å…¥æ€è€ƒæ¨¡å¼å‚æ•°
    let thinking = enable_thinking.unwrap_or(false);
    inject_thinking_params(&mut request_body, &config, thinking);

    let mut req_builder = client
        .post(&url)
        .header("Content-Type", "application/json")
        .body(request_body.to_string());

    if let Some(key) = &config.api_key {
        match config.provider.as_str() {
            "anthropic" => {
                req_builder = req_builder.header("x-api-key", key);
            }
            _ => {
                req_builder = req_builder.header("Authorization", format!("Bearer {}", key));
            }
        }
    }

    let response = req_builder
        .send()
        .await
        .map_err(|e| AppError::AIError(format!("Stream connection failed: {}", e)))?;

    if !response.status().is_success() {
        let status = response.status();
        let error_text = response.text().await.unwrap_or_else(|_| "Unknown".to_string());
        return Err(AppError::AIError(format!(
            "Stream failed ({}): {}", status, error_text
        )));
    }

    stream_sse_chat_completions(response, &req_id, &window).await
}

#[tauri::command]
pub async fn generate_content(
    app: AppHandle,
    author_notes: String,
    current_content: String,
    provider: Option<String>,
    api_key: Option<String>,
    model: Option<String>,
    base_url: Option<String>,
) -> Result<String> {
    let user_prompt = if current_content.is_empty() {
        author_notes.clone()
    } else {
        format!(
            "{}\n\n---\nå‚è€ƒç´ æå¦‚ä¸‹ï¼š\n{}",
            author_notes, current_content
        )
    };

    let messages = vec![
        ChatMessage {
            role: "user".to_string(),
            content: user_prompt,
        },
    ];

    let response = chat(app, messages, provider, api_key, model, base_url, None, None, None).await?;

    Ok(response)
}

#[tauri::command]
pub async fn generate_content_stream(
    app: AppHandle,
    author_notes: String,
    current_content: String,
    provider: Option<String>,
    api_key: Option<String>,
    model: Option<String>,
    base_url: Option<String>,
    window: tauri::Window,
    conversation_history: Option<Vec<ChatMessage>>,
    system_prompt: Option<String>,
    enable_web_search: Option<bool>,
    enable_thinking: Option<bool>,
    request_id: Option<String>,
) -> Result<String> {
    let user_prompt = if current_content.is_empty() {
        author_notes.clone()
    } else {
        format!(
            "{}\n\n---\nå‚è€ƒç´ æå¦‚ä¸‹ï¼š\n{}",
            author_notes, current_content
        )
    };

    // Build messages: only add system message if frontend provided a non-empty system_prompt
    let mut messages: Vec<ChatMessage> = Vec::new();
    if let Some(sp) = system_prompt.filter(|s| !s.trim().is_empty()) {
        messages.push(ChatMessage {
            role: "system".to_string(),
            content: sp,
        });
    }

    // Add conversation history if provided (exclude the last message as it will be the current user prompt)
    if let Some(history) = conversation_history {
        // Take all but the last message if there's history, since the current user message will be added
        let history_len = history.len().saturating_sub(1);
        messages.extend_from_slice(&history[..history_len]);
    }

    // Add current user message
    messages.push(ChatMessage {
        role: "user".to_string(),
        content: user_prompt,
    });

    chat_stream(app, messages, provider, api_key, model, base_url, window, enable_web_search, enable_thinking, None, None, request_id).await
}

#[tauri::command]
pub async fn test_api_connection(
    app: AppHandle,
    provider: Option<String>,
    api_key: Option<String>,
    model: Option<String>,
    base_url: Option<String>,
) -> Result<String> {
    let config = get_ai_config(&app, provider, api_key, model, base_url);
    let client = reqwest::Client::new();
    let url = format!("{}/chat/completions", config.get_base_url());

    let request_body = json!({
        "messages": [{"role": "user", "content": "Hi"}],
        "model": config.get_default_model(),
        "max_tokens": 5,
        "stream": false
    });

    let mut req_builder = client
        .post(&url)
        .header("Content-Type", "application/json")
        .json(&request_body);

    if let Some(key) = &config.api_key {
        match config.provider.as_str() {
            "anthropic" => {
                req_builder = req_builder.header("x-api-key", key);
            }
            _ => {
                req_builder = req_builder.header("Authorization", format!("Bearer {}", key));
            }
        }
    }

    let response = req_builder
        .timeout(Duration::from_secs(15))
        .send()
        .await
        .map_err(|e| AppError::AIError(format!("è¿æ¥å¤±è´¥: {}", e)))?;

    if response.status().is_success() {
        Ok(format!("è¿æ¥æˆåŠŸï¼æ¨¡å‹: {}", config.get_default_model()))
    } else {
        let status = response.status();
        let error_text = response.text().await.unwrap_or_default();
        Err(AppError::AIError(format!("API è¿”å›é”™è¯¯ ({}): {}", status, error_text)))
    }
}

/// OpenAI Responses API éæµå¼è°ƒç”¨
async fn call_openai_responses(
    config: &AIConfig,
    client: &reqwest::Client,
    messages: &[ChatMessage],
    max_tokens: Option<u32>,
) -> Result<String> {
    let url = format!("{}/responses", config.get_base_url());

    let input: Vec<serde_json::Value> = messages.iter().map(|m| {
        json!({ "role": m.role, "content": m.content })
    }).collect();

    let mut request_body = json!({
        "model": config.get_default_model(),
        "input": input,
        "tools": [{ "type": "web_search" }]
    });

    if let Some(mt) = max_tokens {
        request_body["max_tokens"] = json!(mt);
    }

    let mut req_builder = client
        .post(&url)
        .header("Content-Type", "application/json")
        .json(&request_body);

    if let Some(key) = &config.api_key {
        req_builder = req_builder.header("Authorization", format!("Bearer {}", key));
    }

    let response = req_builder
        .timeout(Duration::from_secs(120))
        .send()
        .await
        .map_err(|e| AppError::AIError(format!("OpenAI Responses API failed: {}", e)))?;

    if !response.status().is_success() {
        let status = response.status();
        let error_text = response.text().await.unwrap_or_else(|_| "Unknown".to_string());
        return Err(AppError::AIError(format!("OpenAI Responses API error ({}): {}", status, error_text)));
    }

    let json_val: serde_json::Value = response.json().await
        .map_err(|e| AppError::AIError(format!("Failed to parse Responses API response: {}", e)))?;

    // ä» output æ•°ç»„ä¸­æå–æ–‡æœ¬å†…å®¹
    let output_text = json_val.get("output_text")
        .and_then(|t| t.as_str())
        .unwrap_or("");

    Ok(output_text.to_string())
}

/// Anthropic Claude Messages API éæµå¼è°ƒç”¨ï¼ˆå¸¦è”ç½‘æœç´¢ï¼‰
async fn call_anthropic_with_search(
    config: &AIConfig,
    client: &reqwest::Client,
    messages: &[ChatMessage],
    max_tokens: Option<u32>,
) -> Result<String> {
    let url = format!("{}/messages", config.get_base_url());

    let mut system_content = String::new();
    let mut api_messages: Vec<serde_json::Value> = Vec::new();

    for msg in messages {
        if msg.role == "system" {
            system_content = msg.content.clone();
        } else {
            api_messages.push(json!({ "role": msg.role, "content": msg.content }));
        }
    }

    let mut request_body = json!({
        "model": config.get_default_model(),
        "max_tokens": max_tokens.unwrap_or(8192),
        "messages": api_messages,
        "tools": [{
            "type": "web_search_20250305",
            "name": "web_search",
            "max_uses": 5
        }]
    });

    if !system_content.is_empty() {
        request_body["system"] = json!(system_content);
    }

    let mut req_builder = client
        .post(&url)
        .header("Content-Type", "application/json")
        .header("anthropic-version", "2023-06-01")
        .header("anthropic-beta", "web-search-2025-03-05")
        .json(&request_body);

    if let Some(key) = &config.api_key {
        req_builder = req_builder.header("x-api-key", key);
    }

    let response = req_builder
        .timeout(Duration::from_secs(120))
        .send()
        .await
        .map_err(|e| AppError::AIError(format!("Anthropic API failed: {}", e)))?;

    if !response.status().is_success() {
        let status = response.status();
        let error_text = response.text().await.unwrap_or_else(|_| "Unknown".to_string());
        return Err(AppError::AIError(format!("Anthropic API error ({}): {}", status, error_text)));
    }

    let json_val: serde_json::Value = response.json().await
        .map_err(|e| AppError::AIError(format!("Failed to parse Anthropic response: {}", e)))?;

    // ä» content æ•°ç»„ä¸­æå–æ–‡æœ¬
    let mut result = String::new();
    if let Some(content_arr) = json_val.get("content").and_then(|c| c.as_array()) {
        for block in content_arr {
            if let Some(block_type) = block.get("type").and_then(|t| t.as_str()) {
                if block_type == "text" {
                    if let Some(text) = block.get("text").and_then(|t| t.as_str()) {
                        result.push_str(text);
                    }
                }
            }
        }
    }

    Ok(result)
}

/// é€šç”¨ SSE æµå¼è§£æï¼ˆOpenAI Chat Completions æ ¼å¼ï¼‰
/// è§£æ choices[0].delta.content å’Œ choices[0].delta.reasoning_content
async fn stream_sse_chat_completions(
    response: reqwest::Response,
    req_id: &str,
    window: &tauri::Window,
) -> Result<String> {
    let mut stream = response.bytes_stream();
    use futures_util::StreamExt;

    let mut full_content = String::new();
    let mut buffer = Vec::new();
    let mut in_reasoning = false;

    while let Some(chunk_result) = stream.next().await {
        if is_stream_cancelled(req_id) {
            break;
        }

        let chunk = chunk_result
            .map_err(|e| AppError::AIError(format!("Stream error: {}", e)))?;

        if buffer.len() + chunk.len() > MAX_BUFFER_SIZE {
            return Err(AppError::AIError("Response too large, exceeded buffer limit".to_string()));
        }

        buffer.extend_from_slice(&chunk);

        while let Some(pos) = buffer.iter().position(|&b| b == b'\n') {
            let line_bytes: Vec<u8> = buffer.drain(..=pos).collect();
            let line_str = String::from_utf8_lossy(&line_bytes);
            let line_str = line_str.trim_end_matches('\n').trim_end_matches('\r');

            if line_str.is_empty() {
                continue;
            }

            if let Some(data) = line_str.strip_prefix("data: ") {
                if data == "[DONE]" {
                    // å¦‚æœè¿˜åœ¨ reasoning çŠ¶æ€ï¼Œå…³é—­ think æ ‡ç­¾
                    if in_reasoning {
                        let _ = window.emit("ai:stream:chunk", json!({
                            "request_id": req_id,
                            "content": "</think>"
                        }));
                        full_content.push_str("</think>");
                        in_reasoning = false;
                    }
                    continue;
                }

                if let Ok(json_val) = serde_json::from_str::<serde_json::Value>(data) {
                    let delta = json_val
                        .get("choices")
                        .and_then(|c| c.get(0))
                        .and_then(|c| c.get("delta"));

                    if let Some(delta) = delta {
                        if is_stream_cancelled(req_id) {
                            break;
                        }

                        // å¤„ç† reasoning_contentï¼ˆQwen/DeepSeek/xAI æ€è€ƒå†…å®¹ï¼‰
                        if let Some(reasoning) = delta.get("reasoning_content").and_then(|r| r.as_str()) {
                            if !reasoning.is_empty() {
                                if !in_reasoning {
                                    // å¼€å§‹æ€è€ƒï¼šå‘é€ <think> å¼€æ ‡ç­¾
                                    let _ = window.emit("ai:stream:chunk", json!({
                                        "request_id": req_id,
                                        "content": "<think>"
                                    }));
                                    full_content.push_str("<think>");
                                    in_reasoning = true;
                                }
                                full_content.push_str(reasoning);
                                let _ = window.emit("ai:stream:chunk", json!({
                                    "request_id": req_id,
                                    "content": reasoning
                                }));
                            }
                        }

                        // å¤„ç† contentï¼ˆæ­£æ–‡å†…å®¹ï¼‰
                        if let Some(content) = delta.get("content").and_then(|c| c.as_str()) {
                            if !content.is_empty() {
                                // å¦‚æœä» reasoning åˆ‡æ¢åˆ° contentï¼Œå…³é—­ think æ ‡ç­¾
                                if in_reasoning {
                                    let _ = window.emit("ai:stream:chunk", json!({
                                        "request_id": req_id,
                                        "content": "</think>"
                                    }));
                                    full_content.push_str("</think>");
                                    in_reasoning = false;
                                }
                                full_content.push_str(content);
                                let _ = window.emit("ai:stream:chunk", json!({
                                    "request_id": req_id,
                                    "content": content
                                }));
                            }
                        }
                    }
                }
            }
        }
    }

    // å®‰å…¨å…³é—­ï¼šå¦‚æœæµç»“æŸæ—¶ä»åœ¨ reasoning çŠ¶æ€
    if in_reasoning {
        let _ = window.emit("ai:stream:chunk", json!({
            "request_id": req_id,
            "content": "</think>"
        }));
        full_content.push_str("</think>");
    }

    Ok(full_content)
}

/// OpenAI Responses API æµå¼è°ƒç”¨ï¼ˆæ”¯æŒå†…ç½® web_search å·¥å…·ï¼‰
async fn stream_openai_responses(
    config: &AIConfig,
    messages: &[ChatMessage],
    req_id: &str,
    window: &tauri::Window,
) -> Result<String> {
    let client = reqwest::Client::new();
    let base_url = config.get_base_url();
    let url = format!("{}/responses", base_url);

    // å°† ChatMessage è½¬æ¢ä¸º Responses API çš„ input æ ¼å¼
    let input: Vec<serde_json::Value> = messages.iter().map(|m| {
        json!({
            "role": m.role,
            "content": m.content
        })
    }).collect();

    let request_body = json!({
        "model": config.get_default_model(),
        "input": input,
        "tools": [{ "type": "web_search" }],
        "stream": true
    });

    let mut req_builder = client
        .post(&url)
        .header("Content-Type", "application/json")
        .body(request_body.to_string());

    if let Some(key) = &config.api_key {
        req_builder = req_builder.header("Authorization", format!("Bearer {}", key));
    }

    let response = req_builder
        .send()
        .await
        .map_err(|e| AppError::AIError(format!("OpenAI Responses API connection failed: {}", e)))?;

    if !response.status().is_success() {
        let status = response.status();
        let error_text = response.text().await.unwrap_or_else(|_| "Unknown".to_string());
        return Err(AppError::AIError(format!(
            "OpenAI Responses API failed ({}): {}", status, error_text
        )));
    }

    // Responses API SSE äº‹ä»¶æ ¼å¼ä¸ Chat Completions ä¸åŒ
    let mut stream = response.bytes_stream();
    use futures_util::StreamExt;

    let mut full_content = String::new();
    let mut buffer = Vec::new();

    while let Some(chunk_result) = stream.next().await {
        if is_stream_cancelled(req_id) {
            break;
        }

        let chunk = chunk_result
            .map_err(|e| AppError::AIError(format!("Stream error: {}", e)))?;

        if buffer.len() + chunk.len() > MAX_BUFFER_SIZE {
            return Err(AppError::AIError("Response too large".to_string()));
        }

        buffer.extend_from_slice(&chunk);

        while let Some(pos) = buffer.iter().position(|&b| b == b'\n') {
            let line_bytes: Vec<u8> = buffer.drain(..=pos).collect();
            let line_str = String::from_utf8_lossy(&line_bytes);
            let line_str = line_str.trim_end_matches('\n').trim_end_matches('\r');

            if line_str.is_empty() {
                continue;
            }

            // Responses API ä½¿ç”¨ "event: xxx" + "data: {}" æ ¼å¼
            if let Some(data) = line_str.strip_prefix("data: ") {
                if data == "[DONE]" {
                    continue;
                }

                if let Ok(json_val) = serde_json::from_str::<serde_json::Value>(data) {
                    let event_type = json_val.get("type").and_then(|t| t.as_str()).unwrap_or("");

                    match event_type {
                        // æ–‡æœ¬å¢é‡è¾“å‡º
                        "response.output_text.delta" => {
                            if let Some(delta) = json_val.get("delta").and_then(|d| d.as_str()) {
                                if !delta.is_empty() && !is_stream_cancelled(req_id) {
                                    full_content.push_str(delta);
                                    let _ = window.emit("ai:stream:chunk", json!({
                                        "request_id": req_id,
                                        "content": delta
                                    }));
                                }
                            }
                        }
                        // æ¨ç†å†…å®¹å¢é‡ï¼ˆreasoning æ¨¡å‹ï¼‰
                        "response.reasoning_summary_text.delta" => {
                            if let Some(delta) = json_val.get("delta").and_then(|d| d.as_str()) {
                                if !delta.is_empty() && !is_stream_cancelled(req_id) {
                                    // åŒ…è£¹ä¸º <think> æ ‡ç­¾
                                    let think_content = format!("<think>{}</think>", delta);
                                    full_content.push_str(&think_content);
                                    let _ = window.emit("ai:stream:chunk", json!({
                                        "request_id": req_id,
                                        "content": think_content
                                    }));
                                }
                            }
                        }
                        _ => {}
                    }
                }
            }
        }
    }

    Ok(full_content)
}

/// Anthropic Claude åŸç”Ÿ Messages API æµå¼è°ƒç”¨ï¼ˆæ”¯æŒ web_search server toolï¼‰
async fn stream_anthropic_with_search(
    config: &AIConfig,
    messages: &[ChatMessage],
    req_id: &str,
    window: &tauri::Window,
) -> Result<String> {
    let client = reqwest::Client::new();
    let base_url = config.get_base_url();
    let url = format!("{}/messages", base_url);

    // åˆ†ç¦» system æ¶ˆæ¯å’Œå¯¹è¯æ¶ˆæ¯ï¼ˆAnthropic æ ¼å¼è¦æ±‚ system åœ¨é¡¶å±‚ï¼‰
    let mut system_content = String::new();
    let mut api_messages: Vec<serde_json::Value> = Vec::new();

    for msg in messages {
        if msg.role == "system" {
            system_content = msg.content.clone();
        } else {
            api_messages.push(json!({
                "role": msg.role,
                "content": msg.content
            }));
        }
    }

    let mut request_body = json!({
        "model": config.get_default_model(),
        "max_tokens": 8192,
        "messages": api_messages,
        "tools": [{
            "type": "web_search_20250305",
            "name": "web_search",
            "max_uses": 5
        }],
        "stream": true
    });

    if !system_content.is_empty() {
        request_body["system"] = json!(system_content);
    }

    let mut req_builder = client
        .post(&url)
        .header("Content-Type", "application/json")
        .header("anthropic-version", "2023-06-01")
        .header("anthropic-beta", "web-search-2025-03-05")
        .body(request_body.to_string());

    if let Some(key) = &config.api_key {
        req_builder = req_builder.header("x-api-key", key);
    }

    let response = req_builder
        .send()
        .await
        .map_err(|e| AppError::AIError(format!("Anthropic API connection failed: {}", e)))?;

    if !response.status().is_success() {
        let status = response.status();
        let error_text = response.text().await.unwrap_or_else(|_| "Unknown".to_string());
        return Err(AppError::AIError(format!(
            "Anthropic API failed ({}): {}", status, error_text
        )));
    }

    // Anthropic SSE æ ¼å¼ï¼ševent: xxx \n data: {} \n\n
    let mut stream = response.bytes_stream();
    use futures_util::StreamExt;

    let mut full_content = String::new();
    let mut buffer = Vec::new();

    while let Some(chunk_result) = stream.next().await {
        if is_stream_cancelled(req_id) {
            break;
        }

        let chunk = chunk_result
            .map_err(|e| AppError::AIError(format!("Stream error: {}", e)))?;

        if buffer.len() + chunk.len() > MAX_BUFFER_SIZE {
            return Err(AppError::AIError("Response too large".to_string()));
        }

        buffer.extend_from_slice(&chunk);

        while let Some(pos) = buffer.iter().position(|&b| b == b'\n') {
            let line_bytes: Vec<u8> = buffer.drain(..=pos).collect();
            let line_str = String::from_utf8_lossy(&line_bytes);
            let line_str = line_str.trim_end_matches('\n').trim_end_matches('\r');

            if line_str.is_empty() {
                continue;
            }

            if let Some(data) = line_str.strip_prefix("data: ") {
                if let Ok(json_val) = serde_json::from_str::<serde_json::Value>(data) {
                    let event_type = json_val.get("type").and_then(|t| t.as_str()).unwrap_or("");

                    match event_type {
                        // æ–‡æœ¬å¢é‡
                        "content_block_delta" => {
                            if let Some(delta) = json_val.get("delta") {
                                let delta_type = delta.get("type").and_then(|t| t.as_str()).unwrap_or("");
                                match delta_type {
                                    "text_delta" => {
                                        if let Some(text) = delta.get("text").and_then(|t| t.as_str()) {
                                            if !text.is_empty() && !is_stream_cancelled(req_id) {
                                                full_content.push_str(text);
                                                let _ = window.emit("ai:stream:chunk", json!({
                                                    "request_id": req_id,
                                                    "content": text
                                                }));
                                            }
                                        }
                                    }
                                    "thinking_delta" => {
                                        if let Some(thinking) = delta.get("thinking").and_then(|t| t.as_str()) {
                                            if !thinking.is_empty() && !is_stream_cancelled(req_id) {
                                                let think_text = format!("<think>{}</think>", thinking);
                                                full_content.push_str(&think_text);
                                                let _ = window.emit("ai:stream:chunk", json!({
                                                    "request_id": req_id,
                                                    "content": think_text
                                                }));
                                            }
                                        }
                                    }
                                    _ => {}
                                }
                            }
                        }
                        _ => {}
                    }
                }
            }
        }
    }

    Ok(full_content)
}

/// æ ¹æ® provider æ³¨å…¥è”ç½‘æœç´¢å‚æ•°ï¼ˆChat Completions å±‚ï¼‰
fn inject_web_search_params(request_body: &mut serde_json::Value, config: &AIConfig) {
    match config.provider.as_str() {
        // GLM: æ™ºè°±è‡ªæœ‰çš„ web_search tool æ ¼å¼
        "glm" | "glm-code" => {
            request_body["tools"] = json!([{
                "type": "web_search",
                "web_search": {
                    "enable": true,
                    "search_engine": "search_pro"
                }
            }]);
        }
        // Qwen: é€šè¿‡ enable_search å‚æ•°å¯ç”¨
        "qwen" => {
            request_body["enable_search"] = json!(true);
        }
        // Kimi: å®˜æ–¹å†…ç½®å·¥å…· $web_search
        "kimi" | "kimi-code" => {
            request_body["tools"] = json!([{
                "type": "builtin_function",
                "function": {
                    "name": "$web_search"
                }
            }]);
        }
        // Gemini: Google Search grounding
        "gemini" => {
            request_body["tools"] = json!([{
                "google_search": {}
            }]);
        }
        // xAI: web_search toolï¼ˆOpenAI å…¼å®¹æ ¼å¼ï¼‰
        "xai" => {
            request_body["tools"] = json!([{
                "type": "web_search"
            }]);
        }
        // DeepSeek/MiniMax: æ— å†…ç½®è”ç½‘æœç´¢ï¼ˆå°†åœ¨ Function Calling é˜¶æ®µé€šè¿‡è‡ªå®šä¹‰å·¥å…·å®ç°ï¼‰
        // OpenAI: éœ€è¦ Responses APIï¼ˆå•ç‹¬å¤„ç†ï¼‰
        // Anthropic: éœ€è¦åŸç”Ÿ Messages APIï¼ˆå•ç‹¬å¤„ç†ï¼‰
        _ => {}
    }
}

/// æ ¹æ® provider æ³¨å…¥æ·±åº¦æ€è€ƒå‚æ•°
fn inject_thinking_params(request_body: &mut serde_json::Value, config: &AIConfig, enabled: bool) {
    match config.provider.as_str() {
        // Qwen: é€šè¿‡ enable_thinking å‚æ•°æ§åˆ¶
        "qwen" => {
            request_body["enable_thinking"] = json!(enabled);
        }
        // GLM (GLM-5/GLM-4.5): é€šè¿‡ thinking.type å‚æ•°æ§åˆ¶
        // GLM-5 é»˜è®¤ disabledï¼ŒGLM-4.5 é»˜è®¤ enabledï¼ˆåŠ¨æ€ï¼‰
        // æ€è€ƒå†…å®¹é€šè¿‡ reasoning_content å­—æ®µè¿”å›
        "glm" | "glm-code" => {
            if enabled {
                request_body["thinking"] = json!({ "type": "enabled" });
            } else {
                request_body["thinking"] = json!({ "type": "disabled" });
            }
        }
        // DeepSeek: deepseek-reasoner è‡ªåŠ¨å¯ç”¨æ€è€ƒï¼Œæ— é¢å¤–å‚æ•°
        // ç”±ç”¨æˆ·åœ¨è®¾ç½®ä¸­é€‰æ‹© reasoner æ¨¡å‹
        "deepseek" => {}
        // Kimi/MiniMax: ä½¿ç”¨ <think> æ ‡ç­¾çš„æ¨¡å‹è‡ªåŠ¨å¯ç”¨æ€è€ƒ
        "kimi" | "kimi-code" | "minimax" | "minimax-code" => {}
        // OpenAI: o3/o4-mini ç­‰æ¨ç†æ¨¡å‹è‡ªåŠ¨å¯ç”¨
        "openai" => {}
        // xAI: Grok æ¨ç†æ¨¡å‹è‡ªåŠ¨å¯ç”¨
        "xai" => {}
        // Gemini: 2.5+ è‡ªåŠ¨å¯ç”¨æ€è€ƒ
        "gemini" => {}
        // Anthropic: Extended Thinking éœ€è¦ç‰¹æ®Šå‚æ•°ï¼ˆåœ¨åŸç”Ÿ API ä¸­å¤„ç†ï¼‰
        "anthropic" => {}
        _ => {}
    }
}

fn get_ai_config(
    _app: &AppHandle,
    provider: Option<String>,
    api_key: Option<String>,
    model: Option<String>,
    base_url: Option<String>,
) -> AIConfig {
    let provider_val = provider.unwrap_or_else(|| {
        std::env::var("AI_PROVIDER").unwrap_or_else(|_| "openai".to_string())
    });

    let api_key_val = api_key.or_else(|| {
        std::env::var("AI_API_KEY").ok()
    });

    let base_url_val = base_url
        .filter(|s| !s.is_empty())
        .or_else(|| std::env::var("AI_BASE_URL").ok());

    AIConfig {
        provider: provider_val,
        api_key: api_key_val,
        base_url: base_url_val,
        model,
    }
}
